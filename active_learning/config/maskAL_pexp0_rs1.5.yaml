# Parameters for maskAL
# python maskAL.py --config active_learning/config/maskAL.yaml
# explanation can be found here: https://git.wur.nl/blok012/maskAL

# folders
weightsroot: "./weights"
resultsroot: "./results"
dataroot: "./datasets/pexp0/repeat_factor_training_sampler_1.5"
traindir: "./datasets/train"
valdir: "./datasets/val"
testdir: "./datasets/test01"

# training-parameters
cuda_visible_devices: '0'
classes: ['healthy', 'damaged', 'matured', 'cateye', 'headrot']
transfer_learning_on_previous_models: False
train_complete_trainset: True
duplicate_initial_model: False
learning_rate: 0.01
warmup_iterations: 500
train_iterations_base: 2500
train_iterations_step_size: 2500
step_image_number: 500
step_ratios: [0.5, 0.8]
eval_period: 500
checkpoint_period: 500
weight_decay: 0.0001
learning_policy: 'steps_with_decay'
gamma: 0.1
train_batch_size: 2
num_workers: 2

# train-sampler
train_sampler: "RepeatFactorTrainingSampler"
minority_classes: ['damaged', 'matured', 'cateye', 'headrot']
repeat_factor_smallest_class: 1.5

# evaluation-parameters
confidence_threshold: 0.5
nms_threshold: 0.01

# active-learning sampling
experiment_name: 'pexp0/repeat_factor_training_sampler_1.5'
strategies: ['uncertainty', 'random', 'certainty']
mode: ['min', ' ', 'mean']
initial_datasize: 100
pool_size: 200
equal_pool_size: True
loops: 12
dropout_probability: 0.5
mcd_iterations: 5
dropout_method: 'head'
al_batch_size: 5
iou_thres: 0.5
resume: False
auto_annotate: False
export_format: labelme
supervisely_meta_json: "./datasets/meta.json"
